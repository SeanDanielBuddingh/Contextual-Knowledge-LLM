

# Exploiting Latent Contextual Knowledge for High-Performance Lightweight LLMs: A Framework for Children's Language

| [Paper (under review)](xx) |

This repo supports the paper "Exploiting Latent Contextual Knowledge for High-Performance Lightweight LLMs: A Framework for Children's Language." 


## Overview

Large-scale language models (LLMs) are effective at generating and simplifying text, but they lack the ability to adjust the level of simplification and associated semantics for specific audiences, such as children, and implementing these adjustments is expensive. This limitation leads some approaches to avoid actively utilizing LLMs and instead rely on human intervention for effective simplification, which restricts their scalability and practical application. To resolve this challenge, we propose a framework for developing a lightweight LLM specifically designed to automatically transform complex texts into a format suitable for children, ensuring appropriate vocabulary, semantics, sentence structure, and simplicity. Central to our framework is a novel method called Teacher-Student Task Separation inspired by offline distillation, which smartly addresses the cost issue of directly tuning LLMs for this purpose. We use an LLM as a teacher model to generate children's language based on prompt engineering with a beginner’s dictionary and children’s stories, without any learning. Then, we train a lightweight LLM as a student model using an advanced dictionary and the data generated by the teacher model. This method, referred to as offline distillation through data, employs task separation to allow the lightweight student model to efficiently capture latent contextual knowledge tailored for children's language and even outperform the LLM, the base for the teacher model, in quantitative and qualitative evaluations including our new proposed informative metric for this topic. Our work provides a cost-effective solution using LLMs to enhance children's comprehension of diverse content and has the potential for adaptation to other purpose-specific LLMs.

## License and Intended Use
We release the resources associated with 'Exploiting Latent Contextual Knowledge for High-Performance Lightweight LLMs: A Framework for Children's Language' in this repository under MIT license.


## Installation
To load models in 4bits with transformers and bitsandbytes, you have to install accelerate and transformers from source and make sure you have the latest version of the bitsandbytes library. After installing PyTorch (follow instructions [here](https://pytorch.org/get-started/locally/)), you can achieve the above with the following command:
```bash
pip install -U -r requirements.txt
```
## How to use
Both short text and long text data are available in /mistral/data , as well as teacher model outputs after t-prompt engineering/cosine similarity filitering, and Llama3 70B inference outputs.
Our evaluation results can be found in /mistral/data/evaluation.
For training refer to /mistral_train.py.
For short text inference refer to /mistral_inference.py.
For long text inference refer to /mistral_articles.py.
For evaluation refer to /mistral/data/Readability.ipynb.

This repo is a modified fork of [QLoRA](https://github.com/artidoro/qlora). Please refer to their repo for more information on QLoRA.
